{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4287a4c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ðŸ“˜ Use Case â€” Day 2  \n",
    "**HDFS â†’ Spark â†’ Kafka â†’ SQL Server**\n",
    "\n",
    "Panduan ini berisi langkah-langkah **Hands-on pipeline** dari **HDFS** ke **Kafka** dengan **Spark (batch/cron)**, lalu masuk ke **SQL Server** via **Kafka Connect JDBC Sink**.\n",
    "\n",
    "Semua perintah dijalankan di **Terminal**; notebook ini hanya **panduan dokumentasi**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad18023",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ”· Topology\n",
    "Berikut adalah arsitektur alur data yang akan dibangun:\n",
    "\n",
    "![](imgage/hdfs_spark_kafka_sqlserver.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ec9be",
   "metadata": {},
   "source": [
    "## 1) Prerequisites & Environment\n",
    "\n",
    "- **Cluster**: CDP production-like, Kerberos & TLS aktif.  \n",
    "- **Access**: Sudah `kinit` (punya Kerberos ticket).  \n",
    "- **Tools**: HDFS, Spark 3, Kafka, akses SMM.  \n",
    "- **Security**: JAAS & truststore tersedia (sama seperti UC1).  \n",
    "- **SQL Server**: Akses ke DB `sinkdb`.  \n",
    "\n",
    "ðŸ“Œ Ganti semua `<username>` sesuai user trainee masing-masing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea221d",
   "metadata": {},
   "source": [
    "## 2) Client Security Setup\n",
    "\n",
    "buka terminal: klik logo + di kiri atas, lalu pilih terminal. Kemudian jalankan script berikut:\n",
    "\n",
    "```bash\n",
    "# === JAAS untuk client Kafka ===\n",
    "export KAFKA_OPTS=\"-Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf\"\n",
    "echo \"KAFKA_OPTS=$KAFKA_OPTS\"\n",
    "```\n",
    "\n",
    "```bash\n",
    "# === File config Kafka producer (dipakai CLI & Spark) ===\n",
    "U=\"<username>\"\n",
    "TMP_DIR=\"/tmp/$U\"\n",
    "\n",
    "# bikin folder tmp khusus user (hanya bisa diakses user tsb)\n",
    "install -d -m 700 \"$TMP_DIR\"\n",
    "\n",
    "# copy truststore agar tidak rebutan file global\n",
    "cp -f /tmp/kafka-truststore.jks \"$TMP_DIR/kafka-truststore.jks\"\n",
    "chmod 0644 \"$TMP_DIR/kafka-truststore.jks\"\n",
    "\n",
    "# tulis file config untuk kafka CLI\n",
    "cat > \"$TMP_DIR/producer.properties\" <<EOF\n",
    "security.protocol=SASL_SSL\n",
    "sasl.mechanism=GSSAPI\n",
    "sasl.kerberos.service.name=kafka\n",
    "ssl.truststore.location=$TMP_DIR/kafka-truststore.jks\n",
    "ssl.truststore.password=changeit\n",
    "ssl.endpoint.identification.algorithm=\n",
    "acks=all\n",
    "linger.ms=50\n",
    "batch.size=32768\n",
    "max.in.flight.requests.per.connection=1\n",
    "retries=5\n",
    "delivery.timeout.ms=120000\n",
    "EOF\n",
    "\n",
    "# jadikan variabel default buat perintah2 CLI Kafka\n",
    "export KAFKA_CFG=\"$TMP_DIR/producer.properties\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8cc4e",
   "metadata": {},
   "source": [
    "## 3) Siapkan Data Sumber di HDFS\n",
    "\n",
    "Contoh data JSON per-user:\n",
    "\n",
    "```bash\n",
    "SRC_DIR=\"/uc2/<username>/uc2_src\"\n",
    "hdfs dfs -mkdir -p \"$SRC_DIR\"\n",
    "\n",
    "cat > /tmp/uc2_sample.json <<'JSON'\n",
    "{\"id\": 1, \"name\": \"budi\"}\n",
    "{\"id\": 2, \"name\": \"eko\"}\n",
    "{\"id\": 3, \"name\": \"dina\"}\n",
    "JSON\n",
    "\n",
    "hdfs dfs -put -f /tmp/uc2_sample.json \"$SRC_DIR/\"\n",
    "hdfs dfs -ls -h \"$SRC_DIR\" || true\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b0cce",
   "metadata": {},
   "source": [
    "## 4) Create Topic (Per-User)\n",
    "\n",
    "Gunakan topik per-user agar tidak tabrakan: `usecase2.<username>`\n",
    "\n",
    "```bash\n",
    "TOPIC=\"usecase2.<username>\"\n",
    "\n",
    "kafka-topics --bootstrap-server djp-training-sbox.dla-dataplatform.internal:9093 \\\n",
    "  --command-config \"$KAFKA_CFG\" \\\n",
    "  --create --if-not-exists --topic \"$TOPIC\" --partitions 1 --replication-factor 1\n",
    "\n",
    "# Cek\n",
    "kafka-topics --bootstrap-server djp-training-sbox.dla-dataplatform.internal:9093 \\\n",
    "  --command-config \"$KAFKA_CFG\" --list | grep usecase2.<username> || true\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34e6f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5) Script Spark Batch Producer: HDFS â†’ Kafka\n",
    "\n",
    "Script yang dipakai: **`spark_batch_to_kafka.py`** pertama edit <username> yang ada pada script dibawah ini dengan username masing masing. setelah itu jalankan di terminal.\n",
    "    \n",
    "```bash\n",
    "install -d -m 755 /home/<username>/labs/02_hdfs_spark_kafka_sqlserver\n",
    "\n",
    "cat >/home/<username>/labs/02_hdfs_spark_kafka_sqlserver/spark_batch_to_kafka.py <<'PY'\n",
    "#!/usr/bin/env python3\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "USERNAME = \"<username>\"                                    \n",
    "SRC_DIR  = f\"/uc2/<username>/uc2_src\"\n",
    "TOPIC    = f\"usecase2.<username>\"                       \n",
    "BROKERS  = \"djp-training-sbox.dla-dataplatform.internal:9093\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(f\"uc2-batch-schemaful-<username>\")\n",
    "         .getOrCreate())\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"id\",   T.IntegerType(), True),\n",
    "    T.StructField(\"name\", T.StringType(),  True),\n",
    "])\n",
    "\n",
    "# READ & simple TRANSFORM\n",
    "df = (spark.read.schema(schema).json(SRC_DIR)\n",
    "        .withColumn(\"name\", F.upper(F.trim(\"name\")))\n",
    "        .dropna(subset=[\"id\"])\n",
    "        .coalesce(1))\n",
    "\n",
    "# ==== Connect envelope: {\"schema\": {...}, \"payload\": {...}} ====\n",
    "connect_schema = F.struct(\n",
    "    F.lit(\"struct\").alias(\"type\"),\n",
    "    F.array(\n",
    "        F.struct(F.lit(\"id\").alias(\"field\"),   F.lit(\"int32\").alias(\"type\"),  F.lit(False).alias(\"optional\")),\n",
    "        F.struct(F.lit(\"name\").alias(\"field\"), F.lit(\"string\").alias(\"type\"), F.lit(True).alias(\"optional\"))\n",
    "    ).alias(\"fields\"),\n",
    "    F.lit(False).alias(\"optional\")\n",
    ")\n",
    "\n",
    "payload = F.struct(\n",
    "    F.col(\"id\").cast(\"int\").alias(\"id\"),\n",
    "    F.col(\"name\").alias(\"name\")\n",
    ")\n",
    "\n",
    "value_connect_json = F.to_json(F.struct(\n",
    "    connect_schema.alias(\"schema\"),\n",
    "    payload.alias(\"payload\")\n",
    "))\n",
    "\n",
    "out = df.select(\n",
    "    F.col(\"id\").cast(\"string\").alias(\"key\"),  # key bebas; PK akan diambil dari VALUE\n",
    "    value_connect_json.alias(\"value\")\n",
    ")\n",
    "# ===============================================================\n",
    "\n",
    "# WRITE ke Kafka\n",
    "(out.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "   .write\n",
    "   .format(\"kafka\")\n",
    "   .option(\"kafka.bootstrap.servers\", BROKERS)\n",
    "   .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "   .option(\"kafka.sasl.mechanism\", \"GSSAPI\")\n",
    "   .option(\"kafka.sasl.kerberos.service.name\", \"kafka\")\n",
    "   .option(\"kafka.ssl.truststore.location\", \"/tmp/<username>/kafka-truststore.jks\")\n",
    "   .option(\"kafka.ssl.truststore.password\", \"changeit\")\n",
    "   .option(\"kafka.ssl.endpoint.identification.algorithm\", \"\")\n",
    "   .option(\"topic\", TOPIC)\n",
    "   .save())\n",
    "\n",
    "spark.stop()\n",
    "PY\n",
    "\n",
    "chmod +x /home/<username>/labs/02_hdfs_spark_kafka_sqlserver/spark_batch_to_kafka.py\n",
    "```\n",
    "    \n",
    "    \n",
    "Uji pada terminal (Terminal):\n",
    "```bash\n",
    "mkdir -p /tmp/<username>/spark\n",
    "nice -n 10 ionice -c2 -n7 \\\n",
    "spark3-submit \\\n",
    "  --master local[1] \\\n",
    "  --conf spark.driver.memory=1g \\\n",
    "  --conf spark.sql.shuffle.partitions=2 \\\n",
    "  --conf spark.default.parallelism=1 \\\n",
    "  --conf spark.local.dir=/tmp/<username>/spark \\\n",
    "  --conf spark.driver.extraJavaOptions=\"-Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf\" \\\n",
    "  --conf spark.executor.extraJavaOptions=\"-Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf\" \\\n",
    "  /home/<username>/labs/02_hdfs_spark_kafka_sqlserver/spark_batch_to_kafka.py\n",
    "  \n",
    "#cek isi consumer\n",
    "kafka-console-consumer \\\n",
    "  --bootstrap-server djp-training-sbox.dla-dataplatform.internal:9093 \\\n",
    "  --consumer.config \"$KAFKA_CFG\" \\\n",
    "  --from-beginning --topic \"usecase2.<username>\" | head -3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ed051-4dc8-458d-900c-c21ca208d9a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 6) Cron untuk Spark batch\n",
    "    \n",
    "```bash\n",
    "# GANTI <username> lalu jalankan blok ini sekaligus:\n",
    "# Hapus semua entry lama user <username> jika ada\n",
    "crontab -r 2>/dev/null || true\n",
    "\n",
    "# Buat folder log + ownership (agar bisa nulis)\n",
    "install -d -m 755 /home/<username>/uc2_logs\n",
    "chown -R <username>:<username> /home/<username>/uc2_logs\n",
    "\n",
    "# Pasang crontab untuk <username>\n",
    "( \n",
    "  echo 'SHELL=/bin/bash'\n",
    "  echo 'PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\n",
    "  # heartbeat sederhana: harus bikin /home/<username>/uc2_logs/cron_heartbeat.log dalam 1-2 menit\n",
    "  echo '* * * * * date -Is >> /home/<username>/uc2_logs/cron_heartbeat.log'\n",
    "  # spark job tiap menit, anti-overlap via flock, log ke cron.out\n",
    "  echo '* * * * * KRB5CCNAME=FILE:/tmp/krb5cc_hdfs /usr/bin/flock -n /tmp/uc2.<username>.lock sh -c '\"'\"'mkdir -p /tmp/uc2.<username>.spark /home/<username>/uc2_logs && /usr/bin/spark3-submit --master local[1] --conf spark.driver.memory=1g --conf spark.sql.shuffle.partitions=2 --conf spark.default.parallelism=1 --conf spark.local.dir=/tmp/uc2.<username>.spark --conf spark.ui.enabled=false --conf spark.driver.extraJavaOptions=-Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf --conf spark.executor.extraJavaOptions=-Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf /home/<username>/labs/02_hdfs_spark_kafka_sqlserver/spark_batch_to_kafka.py >>/home/<username>/uc2_logs/cron.out 2>&1'\"'\" \n",
    ") | crontab -\n",
    "```\n",
    "    \n",
    "    \n",
    "Verifikasi entri:\n",
    "```bash\n",
    "crontab -l\n",
    "\n",
    "# optional untuk cek log:\n",
    "tail -n 5 /home/<username>/uc2_logs/cron_heartbeat.log || true\n",
    "tail -n 50 /home/<username>/uc2_logs/cron.out || true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40782133",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7) MSSQL Sink Connector (Per-User)\n",
    "\n",
    "Buka **SMM â†’ Connectors â†’ Add Connector â†’ JDBC Sink Connector**, isi seperti berikut.\n",
    "\n",
    "```properties\n",
    "{\n",
    "  \"name\": \"MSSQL_SINK_<username>\",\n",
    "  \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n",
    "  \"tasks.max\": \"1\",\n",
    "\n",
    "  \"topics\": \"usecase2.<username>\",\n",
    "\n",
    "  \"connection.url\": \"jdbc:sqlserver://djp-training-sbox.dla-dataplatform.internal:1433;databaseName=sinkdb;encrypt=true;trustServerCertificate=true;\",\n",
    "  \"connection.user\": \"sink_user\",\n",
    "  \"connection.password\": \"S1nk!User#2025\",\n",
    "  \"dialect.name\": \"SqlServerDatabaseDialect\",\n",
    "\n",
    "  \"table.name.format\": \"usecase2_<username>\",\n",
    "\n",
    "  \"insert.mode\": \"upsert\",\n",
    "  \"pk.mode\": \"record_value\",\n",
    "  \"pk.fields\": \"id\",\n",
    "\n",
    "  \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n",
    "  \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "  \"value.converter.schemas.enable\": \"true\",\n",
    "\n",
    "  \"consumer.override.group.id\": \"connect-MSSQL_SINK_<username>.v2\",\n",
    "  \"consumer.override.auto.offset.reset\": \"earliest\",\n",
    "  \"consumer.override.security.protocol\": \"SASL_SSL\",\n",
    "  \"consumer.override.sasl.mechanism\": \"GSSAPI\",\n",
    "  \"consumer.override.sasl.kerberos.service.name\": \"kafka\",\n",
    "  \"consumer.override.ssl.truststore.location\": \"/tmp/kafka-truststore.jks\",\n",
    "  \"consumer.override.ssl.truststore.password\": \"changeit\",\n",
    "  \"consumer.override.ssl.endpoint.identification.algorithm\": \"\",\n",
    "\n",
    "  \"errors.tolerance\": \"all\",\n",
    "  \"errors.log.enable\": \"true\",\n",
    "  \"errors.log.include.messages\": \"true\",\n",
    "  \"errors.deadletterqueue.context.headers.enable\": \"true\",\n",
    "  \"errors.deadletterqueue.topic.name\": \"usecase2.<username>.DLQ\",\n",
    "  \"errors.deadletterqueue.topic.partitions\": \"1\",\n",
    "  \"errors.deadletterqueue.topic.replication.factor\": \"1\",\n",
    "\n",
    "  \"auto.create\": \"true\",\n",
    "  \"auto.evolve\": \"true\",\n",
    "  \"secret.properties\": \"connection.password\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c3af8-9fb2-47d1-ad32-2ef1f3719ce8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8) Validasi di SQL Server\n",
    "\n",
    "Buka **`Terminal`**\n",
    "Gunakan **mssql-tools** (dalam container) untuk mengecek tabel per-user di SQL Server.  \n",
    "ðŸ“Œ Ganti `<username>` sesuai user masing-masing.\n",
    "\n",
    "### SQL Server Validation Commands\n",
    "\n",
    "```bash\n",
    "docker run --rm --network host mcr.microsoft.com/mssql-tools \\\n",
    "  /opt/mssql-tools/bin/sqlcmd \\\n",
    "  -S djp-training-sbox.dla-dataplatform.internal,1433 -d sinkdb \\\n",
    "  -U sink_user -P 'S1nk!User#2025' -C -W -s '|' -h -1 \\\n",
    "  -Q \"SELECT TOP(10) id, name FROM dbo.usecase2_<username> ORDER BY TRY_CAST(id AS INT) DESC;\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feda161-03da-4d01-96ec-5ee823878763",
   "metadata": {},
   "source": [
    "## 9) Test tambahkan file baru\n",
    "```bash\n",
    "# 1) Tambah data baru (pakai nama file unik)\n",
    "cat >/tmp/uc2_more.json <<'JSON'\n",
    "{\"id\": 4, \"name\": \"tono\"}\n",
    "{\"id\": 5, \"name\": \"dori\"}\n",
    "JSON\n",
    "\n",
    "ts=$(date +%s)\n",
    "hdfs dfs -put -f /tmp/uc2_more.json \"/uc2/<username>/uc2_src/uc2_more_${ts}.json\"\n",
    "\n",
    "# 2) SQL watcher (cek baris baru muncul)\n",
    "watch -n 2 \"\n",
    "docker run --rm --network host mcr.microsoft.com/mssql-tools \\\n",
    "  /opt/mssql-tools/bin/sqlcmd \\\n",
    "  -S djp-training-sbox.dla-dataplatform.internal,1433 -d sinkdb \\\n",
    "  -U sink_user -P 'S1nk!User#2025' -C -W -s '|' -h -1 \\\n",
    "  -Q \\\"SELECT TOP(10) id, name FROM dbo.usecase2_<username> ORDER BY TRY_CAST(id AS INT) DESC;\\\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a45d5a-36d1-4c0e-a4b7-d5cf38d10504",
   "metadata": {},
   "source": [
    "## 10) Clean up\n",
    "\n",
    "```bash\n",
    "set -euo pipefail\n",
    "\n",
    "U=\"<username>\"                        # <<< pastikan benar\n",
    "USER_NAME=\"<username>\"          # <<< kalau perlu\n",
    "\n",
    "# 1) Hapus baris cron (user saat ini)\n",
    "crontab -l 2>/dev/null | sed '/run_uc2_spark_cron\\.sh/d' | crontab - || true\n",
    "\n",
    "# 2) Matikan Spark yang tepat (batasi ke path & user)\n",
    "pgrep -fu \"$USER_NAME\" \"/home/$USER_NAME/labs/02_hdfs_spark_kafka_sqlserver/spark_batch_to_kafka.py\" >/dev/null 2>&1 && \\\n",
    "  pkill -fu \"$USER_NAME\" \"/home/$USER_NAME/labs/02_hdfs_spark_kafka_sqlserver/spark_batch_to_kafka.py\" || true\n",
    "\n",
    "# 3) Bersihkan artefak lokal (pastikan $U tidak kosong & path benar)\n",
    "[[ -n \"${U}\" ]] || { echo \"Env U kosong, abort\"; exit 1; }\n",
    "rm -rf \"/tmp/${U}/spark\" \"/tmp/${U}/locks\" || true\n",
    "\n",
    "# Kalau log kamu di home user <username>:\n",
    "rm -rf \"/home/${USER_NAME}/uc2_logs\" || true\n",
    "# (atau kalau memang log per-user: /home/$U/uc2_logs)\n",
    "# rm -rf \"/home/${U}/uc2_logs\" || true\n",
    "\n",
    "# 4) Bersihkan source HDFS (opsional)\n",
    "hdfs dfs -test -d \"/uc2/${U}/uc2_src\" 2>/dev/null && \\\n",
    "  hdfs dfs -rm -r -f \"/uc2/${U}/uc2_src/*\" || true\n",
    "\n",
    "# 5) Reset tabel SQL Server (pastikan nama tabel benar)\n",
    "docker run --rm --network host mcr.microsoft.com/mssql-tools \\\n",
    "  /opt/mssql-tools/bin/sqlcmd \\\n",
    "  -S djp-training-sbox.dla-dataplatform.internal,1433 -d sinkdb \\\n",
    "  -U sink_user -P 'S1nk!User#2025' -C -Q \"IF OBJECT_ID('dbo.usecase2_${U}','U') IS NOT NULL TRUNCATE TABLE dbo.usecase2_${U};\"\n",
    "  \n",
    "#Penting untuk Kafka & Connector\n",
    "\n",
    "Agar connector konsumsi lagi dari awal, lakukan salah satu:\n",
    "\n",
    "Paling gampang: ganti consumer.override.group.id di connector (mis. tambahkan .._v2), lalu Save/Restart connector.\n",
    "\n",
    "Atau pakai topik baru (mis. usecase2.<username>.v2) dan update:\n",
    "\n",
    "TOPIC di spark_batch_to_kafka.py\n",
    "\n",
    "topics di connector\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
