# HDFS â†’ Spark Batch â†’ Kafka â†’ SQL Server

## ğŸ“– Overview
This project demonstrates how **Apache Spark (batch)** reads JSON data, applies simple transformations, and publishes messages into **Apache Kafka** in **schemaful envelope format** (mimicking Kafka Connect style: `{"schema": {...}, "payload": {...}}`).  
This approach is useful for bootstrapping Kafka topics with initial data or preparing data for downstream consumers.

---
 
## ğŸ— Architecture
<img src="images/hdfs_spark_kafka_sqlserver.jpg" alt="Spark Batch to Kafka" width="800"/>

**Flow:**
1. **Source JSON** â†’ input dataset (e.g. `id:int`, `name:string`).  
2. **Spark Batch Job** â†’ clean & transform (trim, uppercase, drop nulls).  
3. **Schemaful envelope** â†’ Spark wraps each record into Connect-style `schema` + `payload`.  
4. **Kafka Topic** â†’ receives structured events.  
5. (Optional) **Downstream** â†’ HDFS Sink / JDBC Sink.

---

## ğŸ›  Tech Stack
- Apache Spark 3 (batch mode)  
- Apache Kafka (SASL/Kerberos + TLS enabled)  
- JDK 8/11  
- (Optional) Cron for scheduling jobs  

---

## âš¡ Key Features
- Simple ETL with Spark (cleaning & uppercase transform).  
- Publishes records to Kafka in **schemaful envelope** (compatible with Kafka Connect).  
- Secure Kafka connection using **Kerberos + TLS** (JAAS & krb5 configs included as templates).  

---

## ğŸš€ Components
- **Spark Batch Publisher**  
  Located at [`spark/spark_batch_to_kafka.py`](spark/spark_batch_to_kafka.py).  
  Reads JSON, transforms, wraps into schemaful envelope, and publishes to Kafka.  

- **Security Configs**  
  - [`configs/kafka_client_jaas.conf`](configs/kafka_client_jaas.conf) â†’ JAAS template for Kafka client.  
  - [`configs/krb5.conf`](configs/krb5.conf) â†’ Kerberos realm & domain mapping template.  

- **Notebook (Optional)**  
  [`notebooks/UC2_Spark_Cron.ipynb`](notebooks/UC2_Spark_Cron.ipynb) â€“ shows how to schedule the Spark job periodically with cron.

---

## ğŸ“‚ Repository Structure
```text
hdfs-spark-kafka-sql-server/
â”œâ”€â”€ README.md
â”œâ”€â”€ images/
â”‚   â””â”€â”€ hdfs_spark_kafka_sqlserver.jpg
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ spark_batch_to_kafka.py
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ kafka_client_jaas.conf
â”‚   â””â”€â”€ kafka-truststore.jks
â”‚   â””â”€â”€ krb5.conf
â”‚   â””â”€â”€ mssql-sink.json
â”œâ”€â”€ sample_data/
â”‚   â””â”€â”€ input.jsonl
â”‚   â””â”€â”€ output_envelope.json
â””â”€â”€ notebooks/
    â””â”€â”€ UC2_Spark_Cron.ipynb
```

## ğŸ§ª Sample Input & Output

- [`sample_data/input.jsonl`](sample_data/input.jsonl) â†’ source JSON file.  
- [`sample_data/output_envelope.json`](sample_data/output_envelope.json) â†’ example schemaful envelope published to Kafka.

**Input JSON (placed in `/uc2/<username>/uc2_src`):**


